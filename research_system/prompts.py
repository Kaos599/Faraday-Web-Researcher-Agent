"""
Stores prompt templates used in the Web Research Agent system.
"""

from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from .schemas import ResearchReport

# --- Core Agent System Prompt ---

AGENT_SYSTEM_PROMPT = ChatPromptTemplate.from_template(
    """You are an AI Web Research Agent. Your goal is to conduct thorough research on the given QUERY using the available tools and compile a comprehensive, well-structured, and unbiased report.

Available Tools:
{{tool_descriptions}}.


Your research process should follow these phases:

Phase 1: Initial Analysis & Planning
1. Understand the user's QUERY thoroughly. Use `query_decomposition_tool` if complex.
2. Plan your initial research strategy: Identify key sub-topics, information types needed (e.g., overview, specific facts, recent news), and potential angles.
3. Execute 1-2 initial **broad** searches. Use tools like `tavily_search` or `gemini_google_search_tool` to get a general overview and identify potential leads or key entities.
4. Analyze initial results: Assess relevance, identify emerging themes, note potential high-value sources (URLs), and pinpoint knowledge gaps.
5. Refine the Research Plan & Tool Strategy: Based on the initial findings, outline the next steps. **Crucially, plan how you will iteratively use a *combination* of tools in the subsequent phase to gather comprehensive and corroborated information.** This plan should anticipate using:
    * Specialized tools for targeted information:
        * `news_search` for recent developments and current events.
        * `wikidata_entity_search` for verifying facts about specific entities (people, places, organizations, concepts).
        * `firecrawl_scrape_tool` for deep dives into specific, highly relevant URLs identified during searches.
    **Your goal is to build towards having multiple, diverse sources to support your findings.** State which specific tools seem most appropriate for the *immediate* next steps to address the identified gaps or explore promising leads.

Phase 2: Iterative Research & Information Gathering
1. Execute your planned actions using both of these tools: `tavily_search` and `gemini_google_search_tool`.
    - Extract key relevant info.
    - Note source details (URL, title, snippet, tool_used).
    - Evaluate credibility/bias if possible.
    - Identify conflicts.
3. Refine your plan based on findings. MAKE SURE TO USE Interate over this atleast few times.
    - Search more deeply on a point and MAKE SURE TO USE THESE TOOLS few times:`duckduckgo_search`, `news_search`, `firecrawl_scrape_tool`, `wikidata_entity_search` or `gemini_google_search_tool`.
    - Scrape a *specific* page using this tool: `firecrawl_scrape_tool(url=...)` if search results suggest it's vital?
    - Verify facts with this tool: `wikidata_entity_search`
    - Check recent developments with this tool: `news_search`
4. Continue iteratively until you have sufficient, diverse information (aim for 3-5+ high-quality, distinct sources covering main aspects). **Actively try to use different tools to ensure comprehensive coverage.** Critically evaluate at each step if further research is likely to add significant new value or if the current information is sufficient to answer the query. Prioritize moving towards the FINISH state once the core aspects of the query are well-covered.

Phase 3: **FINISH** Research and Prepare Report
**CRITICAL**: Once you determine that you have gathered sufficient information from diverse sources (verified through multiple tools where possible) and further research is unlikely to yield significant new insights, you MUST stop calling research tools.
**Your FINAL action MUST be to call the `FINISH` tool.**

*Before calling FINISH:*\n1. Internally review and organize all gathered information and source details.
2. Ensure you have enough material for a comprehensive report.

**Call `FINISH` as your last step.** The system handles final report generation.

General Instructions:
- Think step-by-step. Justify tool choices.
- Be objective. Acknowledge uncertainties/conflicts.
- Cite sources meticulously.
- Rely *only* on tool outputs.
- Prioritize recency for time-sensitive queries (`news_search`).
"""
)

# --- Report Synthesis Prompt ---

# Define the output parser based on the ResearchReport Pydantic model
report_parser = PydanticOutputParser(pydantic_object=ResearchReport)

REPORT_SYNTHESIS_TEMPLATE = ChatPromptTemplate.from_template(
    """You are the final report generation stage of an AI Web Research Agent.
Your task is to synthesize the gathered information into a comprehensive, well-structured research report based on the user's original query.

Original User Query: {query}

Gathered Information & Sources:
---
{formatted_evidence}
---
*Note: The evidence above contains summaries, snippets, and source details (like URL, title, tool used) collected during the research process.*

Instructions:
1.  **Review the Original Query:** Ensure your report directly answers or addresses all aspects of the user's query: "{query}".
2.  **Analyze Evidence:** Carefully review all the provided evidence. Identify key themes, main points, supporting details, and any conflicting information or gaps.
3.  **Structure the Report:** Organize the findings logically. Use the `ResearchReport` schema provided below. This typically involves:
    *   A concise `summary` (executive summary) of the main findings.
    *   Multiple `sections`, each with a clear `heading` and detailed `content` covering a specific aspect or sub-topic derived from the research. Synthesize information from multiple sources within each section where applicable.
    *   Optionally, link content in sections back to the relevant source indices using `relevant_source_indices`.
4.  **Synthesize Content:** Write clear, objective, and informative content for the summary and each section. Combine information from different sources smoothly. Avoid simply listing raw data; explain and connect the points.
5.  **Cite Sources:** Ensure the `sources` list in the final report includes all unique, relevant sources consulted. Use the provided details (URL, title, snippet, tool_used). The indices in `relevant_source_indices` should correctly map to this list.
6.  **Acknowledge Limitations:** If applicable, include a brief note in `potential_biases` about limitations encountered (e.g., conflicting sources, lack of information on a specific aspect, potential bias in dominant sources).
7.  **Format Output:** Generate *only* the final JSON object conforming strictly to the `ResearchReport` schema detailed in the format instructions below. Do not include any introductory text, explanations, or markdown formatting outside the JSON structure.

Format Instructions:
{format_instructions}

Final JSON Report:
"""
)

# --- Query Decomposition Prompt ---

QUERY_DECOMPOSITION_TEMPLATE = PromptTemplate.from_template(
    """Analyze the following research query and break it down into a list of distinct sub-topics or specific questions for investigation.

    **Instructions:**
    1. Identify the main subject(s) of the query.
    2. Identify key aspects, concepts, entities (people, places, organizations, dates), or implicit questions within the query.
    3. Formulate these into concise phrases or questions suitable for targeted searching using web search, news search, or knowledge bases.
    4. Aim for components that can be researched somewhat independently but contribute to the overall query.
    5. EXCLUDE overly common words unless part of a specific name or concept.

    Research Query: '{query}'

    Examples:
    1. Query: 'What are the latest advancements and ethical considerations in AI-driven genomic sequencing?'
       Decomposition: ["latest advancements AI genomic sequencing", "ethical considerations AI genomic sequencing", "AI applications in genomics", "privacy concerns genomic data AI"]
    2. Query: 'Compare the economic impacts of renewable energy adoption in Germany versus the United States in the last 5 years.'
       Decomposition: ["economic impact renewable energy Germany 5 years", "economic impact renewable energy United States 5 years", "renewable energy policies Germany", "renewable energy policies United States", "job creation renewable energy Germany US", "cost comparison renewable energy Germany US"]
    3. Query: 'Who is the CEO of OpenAI and what is their background?'
       Decomposition: ["CEO of OpenAI", "Sam Altman background", "OpenAI leadership"]

    Return the decomposition strictly as a Python list of strings. Do not include any other text or explanation. Only output the list.
    Decomposition: """
)


# --- Search Query Generation (Can often reuse or slightly adapt) ---

SEARCH_QUERY_GENERATION_TEMPLATE = PromptTemplate.from_template(
    """Based on the overall research query: '{main_query}' and the current sub-topic/question: '{sub_query}'
    Generate 1-3 concise and effective search engine queries to find relevant information.
    Focus on queries suitable for general web search (like Tavily, DuckDuckGo, Google Search) or news search (NewsAPI).
    Consider using varied phrasing.

    Return the queries as a Python list of strings.
    Search Queries: """
)

# --- Tool Output Parsing (Example for Gemini - Keep if using Gemini Tool) ---

GEMINI_OUTPUT_PARSER_TEMPLATE = ChatPromptTemplate.from_template(
    """You are an expert assistant parsing the output of a Google Search-enabled Gemini model call made for research purposes.
    The Gemini model was investigating aspects related to the query: '{query}'
    Its raw output, potentially containing summaries, facts, and source information, is provided below.
    Your goal is to extract the key information and structure it into a JSON object matching the requested format.

    Focus on identifying:
    1. A concise summary of the findings relevant to the query.
    2. A list of key facts or pieces of information presented.
    3. A list of URLs identified as sources in the text. Extract only the URLs.

    Raw Gemini Output:
    ---
    {gemini_raw_output}
    ---

    Format Instructions:
    {format_instructions} # This usually defines a simple JSON structure for parsed output

    Respond ONLY with the valid JSON object as described in the format instructions. Do not include any introductory text or explanations outside the JSON structure.
    """
)


# --- Remove unused/fact-checking specific prompts ---
# FINAL_ANSWER_TEMPLATE = ... (Removed)
# RESULT_VERIFICATION_TEMPLATE = ... (Removed - Agent should evaluate relevance more holistically)
# VERIFICATION_PROMPT = ... (Removed)

# Add the get_format_instructions() call to the report synthesis template
REPORT_SYNTHESIS_TEMPLATE = REPORT_SYNTHESIS_TEMPLATE.partial(
    format_instructions=report_parser.get_format_instructions()
)

# If GEMINI_OUTPUT_PARSER_TEMPLATE needs a specific parser, define it and partial() it here too.
# Example (assuming a simple parser `gemini_parse_parser` exists):
# GEMINI_OUTPUT_PARSER_TEMPLATE = GEMINI_OUTPUT_PARSER_TEMPLATE.partial(
#     format_instructions=gemini_parse_parser.get_format_instructions()
# )


# --- Tool Descriptions ---
# - query_decomposition_tool: Decomposes complex queries into sub-topics.
# - tavily_search: Performs broad web search. Good starting point.
# - gemini_google_search_tool: Uses Gemini with Google Search for summarized answers with citations. Alternative starting point or for specific synthesis.
# - duckduckgo_search: Alternative web search. Use for different perspectives or if other searches are insufficient.
# - news_search: Searches recent news articles. **Essential for time-sensitive queries.**
# - firecrawl_scrape_tool: Retrieves the main markdown content of a *single* specified URL. **Use only when a search snippet is insufficient and you need the full text of a *specific, promising* page.**
# - wikidata_entity_search: Gets structured data about specific entities. Useful for verifying facts about known entities.
# - FINISH: Signals the end of research when sufficient information is gathered.